{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do you have CUDA available?\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data (path)\n",
    "dataset_name = '\\dataset'\n",
    "root = os.getcwd() + dataset_name\n",
    "\n",
    "# data (img)\n",
    "img_height = 256\n",
    "img_width = 256\n",
    "channels = 3\n",
    "\n",
    "# training\n",
    "epoch = 0 # epoch to start training from\n",
    "n_epochs = 100 # number of epochs of training\n",
    "batch_size = 1 # size of the batches\n",
    "lr = 0.0002 # adam : learning rate\n",
    "b1 = 0.5 # adam : decay of first order momentum of gradient\n",
    "b2 = 0.999 # adam : decay of first order momentum of gradient\n",
    "decay_epoch = 50 # suggested default : 100 (suggested 'n_epochs' is 200)\n",
    "                 # epoch from which to start lr decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), # Pads the input tensor using the reflection of the input boundary\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_block):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "        \n",
    "        channels = input_shape[0]\n",
    "        \n",
    "        # Initial Convolution Block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        in_features = out_features\n",
    "        \n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_block):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "            \n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2), # --> width*2, heigh*2\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            \n",
    "        # Output Layer\n",
    "        model += [nn.ReflectionPad2d(channels),\n",
    "                  nn.Conv2d(out_features, channels, 7),\n",
    "                  nn.Tanh()\n",
    "                 ]\n",
    "        \n",
    "        # Unpacking\n",
    "        self.model = nn.Sequential(*model) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        channels, height, width = input_shape\n",
    "        \n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height//2**4, width//2**4)\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128,256),\n",
    "            *discriminator_block(256,512),\n",
    "            nn.ZeroPad2d((1,0,1,0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_identity = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (channels, img_height, img_width) # (3,256,256)\n",
    "n_residual_blocks = 9 # suggested default, number of residual blocks in generator\n",
    "\n",
    "G_AB = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "G_BA = GeneratorResNet(input_shape, n_residual_blocks)\n",
    "D_A = Discriminator(input_shape)\n",
    "D_B = Discriminator(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print(f'CUDA Availibility: {cuda}')\n",
    "if cuda:\n",
    "    G_AB = G_AB.cuda()\n",
    "    G_BA = G_BA.cuda()\n",
    "    D_A = D_A.cuda()\n",
    "    D_B = D_B.cuda()\n",
    "    \n",
    "    criterion_GAN.cuda()\n",
    "    criterion_cycle.cuda()\n",
    "    criterion_identity.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02) # reset Conv2d's weight(tensor) with Gaussian Distribution\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0) # reset Conv2d's bias(tensor) with Constant(0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            torch.nn.init.normal_(m.weight.data, 1.0, 0.02) # reset BatchNorm2d's weight(tensor) with Gaussian Distribution\n",
    "            torch.nn.init.constant_(m.bias.data, 0.0) # reset BatchNorm2d's bias(tensor) with Constant(0)\n",
    "\n",
    "G_AB.apply(weights_init_normal)\n",
    "G_BA.apply(weights_init_normal)\n",
    "D_A.apply(weights_init_normal)\n",
    "D_B.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_weights_init_normal(m):\n",
    "    classname =  m.__class__.__name__\n",
    "    print(classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_AB.apply(temp_weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "# lr = 0.0002\n",
    "# b1 = 0.5\n",
    "# b2 = 0.999\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(\n",
    "    D_A.parameters(), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "optimizer_D_B = torch.optim.Adam(\n",
    "    D_B.parameters(), lr=lr, betas=(b1,b2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "        \n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch+self.offset - self.decay_start_epoch)/(self.n_epochs - self.decay_start_epoch)\n",
    "\n",
    "\n",
    "import itertools\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "\n",
    "optimizer_D_A = torch.optim.Adam(\n",
    "    D_A.parameters(), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "optimizer_D_B = torch.optim.Adam(\n",
    "    D_B.parameters(), lr=lr, betas=(b1,b2)\n",
    ")\n",
    "\n",
    "# n_epochs = 10\n",
    "# epoch = 0\n",
    "# decay_epoch = 5\n",
    "\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "lr_scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_A,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D_B,\n",
    "    lr_lambda=LambdaLR(n_epochs, epoch, decay_epoch).step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode='train'):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.files_A = sorted(glob.glob(os.path.join(root+'\\landscapes')+'/*.*')[500:])\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root+'\\paintings')+'/*.*')[500:])\n",
    "        elif self.mode == 'test':\n",
    "            self.files_A = sorted(glob.glob(os.path.join(root+'\\landscapes')+'/*.*')[:500])\n",
    "            self.files_B = sorted(glob.glob(os.path.join(root+'\\paintings')+'/*.*')[:500]) # was 250:301\n",
    "\n",
    "    def  __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
    "        \n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.files_B[np.random.randint(0, len(self.files_B)-1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "        if image_A.mode != 'RGB':\n",
    "            image_A = to_rgb(image_A)\n",
    "        if image_B.mode != 'RGB':\n",
    "            image_B = to_rgb(image_B)\n",
    "            \n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {'A':item_A, 'B':item_B}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "\n",
    "\n",
    "def to_rgb(image):\n",
    "    rgb_image = Image.new(\"RGB\", image.size)\n",
    "    rgb_image.paste(image)\n",
    "    return rgb_image\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(img_height*1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    ImageDataset(root, transforms_=transforms_, unaligned=True),\n",
    "    batch_size=8, # 1\n",
    "    shuffle=True,\n",
    "    num_workers=0 # 3\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(root, transforms_=transforms_, unaligned=True, mode='test'),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images():\n",
    "    \"\"\"show a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    real_A = imgs['A'].type(Tensor) # A : monet\n",
    "    fake_B = G_AB(real_A).detach()\n",
    "    real_B = imgs['B'].type(Tensor) # B : photo\n",
    "    fake_A = G_BA(real_B).detach()\n",
    "    # Arange images along x-axis\n",
    "    real_A = make_grid(real_A, nrow=5, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=5, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=5, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis    \n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)\n",
    "    plt.imshow(image_grid.cpu().permute(1,2,0))\n",
    "    plt.title('Real A vs Fake B | Real B vs Fake A')\n",
    "    plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_imgs = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G_AB.eval() # test mode \n",
    "#G_BA.eval() # test mode\n",
    "#print(temp_imgs['A'].shape)\n",
    "#print(temp_imgs['B'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_real_A = temp_imgs['A'].type(Tensor) # A : monet\n",
    "#temp_fake_B = G_AB(temp_real_A).detach()\n",
    "#temp_real_B = temp_imgs['B'].type(Tensor) # B : photo\n",
    "#temp_fake_A = G_BA(temp_real_B).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(temp_real_A.shape)\n",
    "#print(temp_fake_B.shape)\n",
    "#print(temp_real_B.shape)\n",
    "#print(temp_fake_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_real_A = make_grid(temp_real_A, nrow=5, normalize=True)\n",
    "#temp_real_B = make_grid(temp_real_B, nrow=5, normalize=True)\n",
    "#temp_fake_A = make_grid(temp_fake_A, nrow=5, normalize=True)\n",
    "#temp_fake_B = make_grid(temp_fake_B, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(temp_real_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(temp_real_A.cpu().permute(1,2,0))\n",
    "#plt.title('Real A')\n",
    "#plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(temp_real_A.shape)\n",
    "#print(temp_fake_B.shape)\n",
    "#print(temp_real_B.shape)\n",
    "#print(temp_fake_A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_image_grid = torch.cat((temp_real_A, temp_fake_A, temp_real_B, temp_fake_B), 1)\n",
    "#print(temp_image_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_image_grid.cpu().permute(1,2,0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(temp_image_grid.cpu().permute(1,2,0))\n",
    "#plt.title('Real A | Fake B | Real B | Fake A ')\n",
    "#plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp_image_grid = torch.cat((temp_real_A, temp_fake_A, temp_real_B, temp_fake_B), 1)\n",
    "#print(temp_image_grid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoch, n_epochs):\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        # Set model input\n",
    "        real_A = batch['A'].type(Tensor)\n",
    "        real_B = batch['B'].type(Tensor)\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = Tensor(np.ones((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
    "        fake = Tensor(np.zeros((real_A.size(0), *D_A.output_shape))) # requires_grad = False. Default.\n",
    "        \n",
    "# -----------------\n",
    "# Train Generators\n",
    "# -----------------\n",
    "        G_AB.train() # train mode\n",
    "        G_BA.train() # train mode\n",
    "        \n",
    "        optimizer_G.zero_grad() # Integrated optimizer(G_AB, G_BA)\n",
    "        \n",
    "        # Identity Loss\n",
    "        loss_id_A = criterion_identity(G_BA(real_A), real_A) # If you put A into a generator that creates A with B,\n",
    "        loss_id_B = criterion_identity(G_AB(real_B), real_B) # then of course A must come out as it is.\n",
    "                                                             # Taking this into consideration, add an identity loss that simply compares 'A and A' (or 'B and B').\n",
    "        loss_identity = (loss_id_A + loss_id_B)/2\n",
    "        \n",
    "        # GAN Loss\n",
    "        fake_B = G_AB(real_A) # fake_B is fake-photo that generated by real monet-drawing\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid) # tricking the 'fake-B' into 'real-B'\n",
    "        fake_A = G_BA(real_B)\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid) # tricking the 'fake-A' into 'real-A'\n",
    "        \n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA)/2\n",
    "        \n",
    "        # Cycle Loss\n",
    "        recov_A = G_BA(fake_B) # recov_A is fake-monet-drawing that generated by fake-photo\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A) # Reduces the difference between the restored image and the real image\n",
    "        recov_B = G_AB(fake_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "        \n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B)/2\n",
    "        \n",
    "# ------> Total Loss\n",
    "        loss_G = loss_GAN + (10.0*loss_cycle) + (5.0*loss_identity) # multiply suggested weight(default cycle loss weight : 10, default identity loss weight : 5)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "# -----------------\n",
    "# Train Discriminator A\n",
    "# -----------------\n",
    "        optimizer_D_A.zero_grad()\n",
    "    \n",
    "        loss_real = criterion_GAN(D_A(real_A), valid) # train to discriminate real images as real\n",
    "        loss_fake = criterion_GAN(D_A(fake_A.detach()), fake) # train to discriminate fake images as fake\n",
    "        \n",
    "        loss_D_A = (loss_real + loss_fake)/2\n",
    "        \n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "# -----------------\n",
    "# Train Discriminator B\n",
    "# -----------------\n",
    "        optimizer_D_B.zero_grad()\n",
    "    \n",
    "        loss_real = criterion_GAN(D_B(real_B), valid) # train to discriminate real images as real\n",
    "        loss_fake = criterion_GAN(D_B(fake_B.detach()), fake) # train to discriminate fake images as fake\n",
    "        \n",
    "        loss_D_B = (loss_real + loss_fake)/2\n",
    "        \n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "        \n",
    "# ------> Total Loss\n",
    "        loss_D = (loss_D_A + loss_D_B)/2\n",
    "    \n",
    "# -----------------\n",
    "# Show Progress\n",
    "# -----------------\n",
    "        if (i+1) % 50 == 0:\n",
    "            # sample_images()\n",
    "            print('[Epoch %d/%d] [Batch %d/%d] [D loss : %f] [G loss : %f - (adv : %f, cycle : %f, identity : %f)]'\n",
    "                    %(epoch+1,n_epochs,       # [Epoch -]\n",
    "                      i+1,len(dataloader),   # [Batch -]\n",
    "                      loss_D.item(),       # [D loss -]\n",
    "                      loss_G.item(),       # [G loss -]\n",
    "                      loss_GAN.item(),     # [adv -]\n",
    "                      loss_cycle.item(),   # [cycle -]\n",
    "                      loss_identity.item(),# [identity -]\n",
    "                     ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G_AB = models.vgg16(pretrained=True)\n",
    "torch.save(G_AB.state_dict(), 'land2paint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_BA = models.vgg16(pretrained=True)\n",
    "torch.save(G_BA.state_dict(), 'paint2land.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_A = models.vgg16(pretrained=True)\n",
    "torch.save(D_A.state_dict(), 'fakepaint2land.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_B = models.vgg16(pretrained=True)\n",
    "torch.save(D_B.state_dict(), 'fakeland2paint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "G_AB = \n",
    "G_AB = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
    "G_AB.load_state_dict(torch.load('land2paint.pth'))\n",
    "in_ftr = G_AB.fc.in_features\n",
    "out_ftr = ([3, 256, 256])\n",
    "G_AB.cuda()\n",
    "G_AB.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    # print('iter : {}  A.size : {}'.format(i,batch['A'].size()))\n",
    "    # print('iter : {}  B.size : {}'.format(i,batch['B'].size()))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n",
      "torch.cuda.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "G_AB.eval()\n",
    "temp_A = batch['A'] # A : monet\n",
    "print(temp_A.size())\n",
    "fake_B = G_AB(temp_A.cuda()).detach()\n",
    "fake_B = fake_B[0]\n",
    "print(fake_B.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img = temp_A[0]\n",
    "print(temp_img.size())\n",
    "G_AB.eval()\n",
    "temp_img=(temp_img+1)/2\n",
    "temp_img = temp_img.cpu().permute(1,2,0).numpy()\n",
    "plt.imshow(temp_img)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img = fake_B.squeeze()\n",
    "G_AB.eval()\n",
    "temp_img=(temp_img+1)/2\n",
    "temp_img = temp_img.cpu().permute(1,2,0).numpy()\n",
    "plt.imshow(temp_img)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_B = batch['B'].type(Tensor) # B\n",
    "fake_A = G_BA(temp_B).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img = temp_B.squeeze()\n",
    "G_BA.eval()\n",
    "temp_img=(temp_img+1)/2\n",
    "temp_img = temp_img.cpu().permute(1,2,0).numpy()\n",
    "plt.imshow(temp_img)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_img = fake_A.squeeze()\n",
    "G_BA.eval()\n",
    "temp_img=(temp_img+1)/2\n",
    "temp_img = temp_img.cpu().permute(1,2,0).numpy()\n",
    "plt.imshow(temp_img)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_A.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_A.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_A.size(0) # batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor(np.ones((temp_A.size(0), *D_A.output_shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor(np.ones((temp_A.size(0), *D_A.output_shape))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, d=128):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.deconv1 = nn.ConvTranspose2d(100, d*8, 4, 1, 0)\n",
    "#         self.deconv1_bn = nn.BatchNorm2d(d*8)\n",
    "#         self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
    "#         self.deconv2_bn = nn.BatchNorm2d(d*4)\n",
    "#         self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
    "#         self.deconv3_bn = nn.BatchNorm2d(d*2)\n",
    "#         self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
    "#         self.deconv4_bn = nn.BatchNorm2d(d)\n",
    "#         self.deconv5 = nn.ConvTranspose2d(d, 1, 4, 2, 1)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         # x = F.relu(self.deconv1(input))\n",
    "#         x = F.relu(self.deconv1_bn(self.deconv1(input)))\n",
    "#         x = F.relu(self.deconv2_bn(self.deconv2(x)))\n",
    "#         x = F.relu(self.deconv3_bn(self.deconv3(x)))\n",
    "#         x = F.relu(self.deconv4_bn(self.deconv4(x)))\n",
    "#         x = torch.tanh(self.deconv5(x))\n",
    "\n",
    "#         return x\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, d=128):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n",
    "#         self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n",
    "#         self.conv2_bn = nn.BatchNorm2d(d*2)\n",
    "#         self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n",
    "#         self.conv3_bn = nn.BatchNorm2d(d*4)\n",
    "#         self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n",
    "#         self.conv4_bn = nn.BatchNorm2d(d*8)\n",
    "#         self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         x = F.leaky_relu(self.conv1(input), 0.2)\n",
    "#         x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n",
    "#         x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n",
    "#         x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n",
    "#         x = torch.sigmoid(self.conv5(x))\n",
    "\n",
    "#         return x\n",
    "\n",
    "    \n",
    "# generator = Generator()\n",
    "# discriminator = Discriminator()\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if True and torch.cuda.is_available() else \"cpu\")\n",
    "# generator = generator.to(device)\n",
    "# discriminator = discriminator.to(device)\n",
    "\n",
    "# num_params_gen = sum(p.numel() for p in generator.parameters() if p.requires_grad)\n",
    "# num_params_disc = sum(p.numel() for p in discriminator.parameters() if p.requires_grad)\n",
    "# print('Number of parameters for generator: %d and discriminator: %d' % (num_params_gen, num_params_disc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GAN training can be unstable. In this case, the strong momentum\n",
    "# # for the gradient prevents convergence. One possible explanation is that the\n",
    "# # strong momentum does not allow the two players in the adversarial game to react\n",
    "# # to each other quickly enough. Decreasing beta1 (the exponential decay for the\n",
    "# # gradient moving average in [0,1], lower is faster decay) from the default 0.9\n",
    "# # to 0.5 allows for quicker reactions.\n",
    "# gen_optimizer = torch.optim.Adam(params=generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "# disc_optimizer = torch.optim.Adam(params=discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# # set to training mode\n",
    "# generator.train()\n",
    "# discriminator.train()\n",
    "\n",
    "# gen_loss_avg = []\n",
    "# disc_loss_avg = []\n",
    "\n",
    "# print('Training ...')\n",
    "# for epoch in range(num_epochs):\n",
    "#     gen_loss_avg.append(0)\n",
    "#     disc_loss_avg.append(0)\n",
    "#     num_batches = 0\n",
    "    \n",
    "#     for image_batch, _ in ltrain_dataloader:\n",
    "        \n",
    "#         # get dataset image and create real and fake labels for use in the loss\n",
    "#         image_batch = image_batch.to(device)\n",
    "#         label_real = torch.ones(image_batch.size(0), device=device)\n",
    "#         label_fake = torch.zeros(image_batch.size(0), device=device)\n",
    "\n",
    "#         # generate a batch of images from samples of the latent prior\n",
    "#         latent = torch.randn(image_batch.size(0), 3, 400 , 400, device=device)\n",
    "#         fake_image_batch = generator(latent)\n",
    "        \n",
    "#         # train discriminator to correctly classify real and fake\n",
    "#         # (detach the computation graph of the generator and the discriminator,\n",
    "#         # so that gradients are not backpropagated into the generator)\n",
    "#         real_pred = discriminator(image_batch).squeeze()\n",
    "#         fake_pred = discriminator(fake_image_batch.detach()).squeeze()\n",
    "#         disc_loss = 0.5 * (\n",
    "#             F.binary_cross_entropy(real_pred, label_real) +\n",
    "#             F.binary_cross_entropy(fake_pred, label_fake))\n",
    "        \n",
    "#         disc_optimizer.zero_grad()\n",
    "#         disc_loss.backward()\n",
    "#         disc_optimizer.step()\n",
    "        \n",
    "#         # train generator to output an image that is classified as real\n",
    "#         fake_pred = discriminator(fake_image_batch).squeeze()\n",
    "#         gen_loss = F.binary_cross_entropy(fake_pred, label_real)\n",
    "        \n",
    "#         gen_optimizer.zero_grad()\n",
    "#         gen_loss.backward()\n",
    "#         gen_optimizer.step()\n",
    "        \n",
    "#         gen_loss_avg[-1] += gen_loss.item()\n",
    "#         disc_loss_avg[-1] += disc_loss.item()\n",
    "#         num_batches += 1\n",
    "        \n",
    "#     gen_loss_avg[-1] /= num_batches\n",
    "#     disc_loss_avg[-1] /= num_batches\n",
    "#     print('Epoch [%d / %d] average loss generator vs. discrim.: %f vs. %f' %\n",
    "#           (epoch+1, num_epochs, gen_loss_avg[-1], disc_loss_avg[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3196968d684371006099b3d55edeef8ed90365227a30deaef86e5d4aa8519be0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
